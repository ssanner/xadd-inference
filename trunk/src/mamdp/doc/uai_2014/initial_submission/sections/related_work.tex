\section{Related Work}
\label{sec:relatedwork}

Solutions to stochastic games have been proposed from within both
game theory and reinforcement learning. The first algorithm, game theoretic or otherwise, for 
finding a solution to a stochastic game was given by Shapley \cite{Shapley_PotNAoS_1953}.
The algorithm repeatedly calculates a value function $V(s)$ over discrete states
which converges to an optimal value function $V^{*}(s)$, which represents
the expected discounted future reward if both players in the game followed
the game's Nash equilibrium. Shapley's algorithm is in essence an 
extension of the value iteration algorithm to stochastic games. A reinforcement 
learning based solution to stochastic games was first introduced by 
Littman \cite{Littman_ICML_1994}. Littman's algorithm, Minimax-Q,
extends the traditional Q-learning algorithm for MDPs to 
zero-sum discrete stochastic games. The algorithm converges to the 
stochastic game's equilibrium solution. Hu and Wellman \cite{Hu_ICML_1998}
extended Minimax-Q to general-sum games and proved that it converges
to a Nash equilibrium under certain restrictive conditions. Although
both reinforcement learning based algorithms are able to calculate 
equilibrium solutions they are limited to discrete state formulations of
stochastic games. In this paper we calculate exact solutions to 
continuous state formulations of stochastic games, under certain restrictions.
The Dec-MDP \cite{Bernstein_MoOR_2002} framework allows
for decentralised control within continuous state spaces but is limited
to general-sum systems. In this paper we provide the first 
known exact closed-form solution to a subclass of continuous state zero-sum 
stochastic games defined by a piecewise constant reward and piecewise linear transition.

Several techniques have been put forward to tackle continuous state
spaces in MDPs. Li and Littman \cite{Li_AAAI_2005} describe a method
to approximate intermediate piecewise linear value functions by piecewise
constant functions, which results in approximate solutions to continuous
state MDPs. In this paper we use symbolic dynamic programming to
calculate exact solutions to game theoretic domains with continuous state. 

Symbolic dynamic programming techniques have been previously used to calculate
exact solutions to single agent MDPs with both continuous state and actions
in a variety of non-game theoretic domains \cite{Sanner_UAI_2011,Zamani_AAAI_2012}.
In this paper we present the first application of SDP to game-theoretic domains.

%----------------------------------------------------------------------------
%The use of reinforcement learning methods to solve stochastic games
%was introduced by Littman \cite{Littman_ICML_1994}. Under Littman's
%formulation optimal policies can be calculated for discrete state zero-sum 
%stochastic games using an algorithm akin to Q-learning. Hu \cite{Hu_ICML_1998}
%extended Littman's framework into the general-sum case. Whilst both
%approaches provide optimal policies for stochastic games under zero-sum
%and general sum settings, they are limited to discrete state formulations. 
%
%A lazy approximation technique for MDPs which imposes similar restrictions on the
%form of the reward and transition functions was introduced by Li \cite{Li_AAAI_2005}
%and allows for the calculation of approximate solutions. Our approach using
%symbolic dynamic programming calculates exact solutions in game theoretic
%settings. 





