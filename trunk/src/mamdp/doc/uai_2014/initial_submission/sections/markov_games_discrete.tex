\section{Zero-sum Discrete Stochastic Games}
\label{sec:dsg}
Discrete state stochastic games (DSGs) are formally defined by the tuple
$ \left\langle S, A_{1}, \ldots, A_{n}, T, R_{1}, \ldots, R_{n}, h, \gamma\right\rangle$.
$S$ is a set of discrete states and $A_i$ is the action set available to agent 
$i$. T is a transition function $T : S \times A_1 \times \ldots \times A_n \rightarrow \Delta(S)$, 
where $\Delta(S)$ is the set of probability distributions over the state space $S$. 
The reward function $R_i : S \times A_1 \times \ldots \times A_n \rightarrow \mathbb{R}$ 
encodes the individual preferences of agent $i$. The horizon $h$ represents the 
number of decision steps until termination and the discount factor $\gamma \in [0, 1)$ 
is used to discount future rewards. In general, an agent's objective is 
to find a policy, $\pi_i : S \rightarrow \sigma_i(A_i)$ which maximises the expected 
sum of discounted rewards over horizon $h$. Here $\sigma_i(A_i)$ specifies
probability distributions over action set $A_i$. The optimal policy in a 
DSG may be stochastic, that is, it may define a mixed strategy for each state. 

%The goal of each agent in a stochastic game is to maximise its expected 
%discounted future rewards.

%Two agent zero-sum DSGs impose a condition on the reward structure 
%of the game whereby the goals of an agent and its opponent are diametrically
%opposed to one another. Under this restriction the reward structure can 
%be represented by a single reward function. The opponent's
%reward function is simply the opposite of the agent's. 
%Within this game 
%each agent attempts to maximise its expected discounted future rewards 
%in the minimax sense. Since the reward structure is zero-sum it is sufficient to view the 
%opponent as acting to minimise the agent's return.

Zero-sum DSGs are a type of DSG involving two agents with diametrically
opposing goals. Under these conditions the reward structure for the 
game can be represented by a single reward function since an agents
reward function is simply the opposite of their opponent's. The objective
of each agent is to maximise its expected discounted future rewards 
in the minimax sense. That is, each agent views its opponent as
acting to minimise the agent's reward. Zero-sum DSGs can be solved 
using a technique analogous to value iteration for MDPs \cite{Littman_ICML_1994}. 
The value function, $V^{h}(s)$, in this setting can be defined as:

\vspace{-7.5mm}
{\small

%\abovedisplayskip=0pt
\belowdisplayskip=0pt
%\allowdisplaybreaks
\begin{multline}
\label{eq:dsgvfunc}
  V^{h}(s) = \\
  \max_{m \in \sigma_1(A_1)} \hspace{2pt} \min_{o \in \sigma_2(A_2)} \sum_{a_1 \in A_1} \sum_{a_2 \in A_2} Q^{h}(s, a_1, a_2) \cdot m_{a_{1}} \cdot o_{a_{2}}
\end{multline}
}%

where $m$ and $o$ are mixed strategies from $\sigma_1(A_1)$ and
$\sigma_2(A_2)$, respectively. $Q^{h}(s, a_1, a_2)$, the quality of taking action $a_1$ against action $a_2$ in state $s$,
is given by:

\vspace{-6.5mm}
{\small 
%\abovedisplayskip=0pt
\belowdisplayskip=0pt
\begin{multline}
\label{eq:dsgdiscqfunc}
  Q^{h}(s, a_1, a_2) = R(s, a_1, a_2) + \\
  \gamma \cdot \sum_{s' \in S} T(s, a_1, a_2, s') \cdot V^{h-1}(s').
\end{multline}
}%

%It is well known that Equation \eqref{eq:dsgvfunc} can be further simplified to:
Equation \eqref{eq:dsgvfunc} can be further simplified by noting that
since the $\text{min}$ operation is ``inside'' the $\text{max}$, the minimum is achieved
for a deterministic action choice. This observation leads to the following
form:

\vspace{-6.5mm}
{\small 
%\abovedisplayshortskip =100pt
%\belowdisplayshortskip =100pt
\begin{equation}
\label{eq:dsgvfunccompact}
  V^{h}(s) = \max_{m \in \sigma_1(A_1)} \min_{a_2 \in A_2} \sum_{a_1 \in A_1} Q^{h}(s, a_1, a_2) \cdot m_{a_1}.
\end{equation}
}%
\vspace{-6.5mm}

Together Equations \eqref{eq:dsgdiscqfunc} and \eqref{eq:dsgvfunccompact}
define a recursive method to calculate the optimal solution to zero-sum
DSGs. The policy for the opponent can be calculated by applying symmetric
reasoning and the Minimax theorem \cite{Neumann_MA_1928}. 

\subsection{Solution Techniques}
\label{subsec:dsgsolution}

Zero-sum DSGs can be solved via discrete linear optimisation. The value
function in Equation \eqref{eq:dsgvfunccompact} can be reformulated as a linear
program through the following steps:

\begin{enumerate}
  \item Define $V^h(s)$ to be the value of the inner minimisation term in
            Equation \eqref{eq:dsgvfunccompact}. This leads to the following linear program for a known state $s$:
{\small
\abovedisplayskip=10pt
\belowdisplayskip=0pt 
\begin{subequations}
\begin{align}
&\text{maximise}   \  V^{h}(s) \nonumber \\
&\text{subject to}   \nonumber \\
&\  V^{h}(s) = \min_{a_2 \in A_2} \sum_{a_1 \in A_1} Q^{h}(s, a_1, a_2) \cdot m_{a_{1}} \label{eq:dsglpconstraint1} \\
                          &\  \sum_{a_{1} \in A_1} m_{a_{1}} = 1 ; \  m_{a_{1}} \geq 0 \qquad \forall a_{1} \in A_1 \nonumber
\end{align}
\end{subequations}
}%

  \item Replace the equality (=) in constraint \eqref{eq:dsglpconstraint1} with
  $\leq$ by observing that the maximisation of $V^{h}(s)$
  effectively pushes the $\leq$ condition to the = case. This gives: 
{\small 
\abovedisplayskip=8pt
\belowdisplayskip=0pt
\begin{subequations}
\begin{align}
&\text{maximise}   \  V^{h}(s) \nonumber \\
&\text{subject to}   \nonumber \\
&\  V^{h}(s) \leq \min_{a_2 \in A_2} \sum_{a_1 \in A_1} Q^{h}(s, a_1, a_2) \cdot m_{a_{1}} \label{eq:dsglpconstraint2} \\
                          &\  \sum_{a_{1} \in A_1} m_{a_{1}} = 1 ; \  m_{a_{1}} \geq 0 \qquad \forall a_{1} \in A_1 \nonumber
\end{align}
\end{subequations}
}%
  
  \item Remove the minimisation operator in constraint \eqref{eq:dsglpconstraint2}
            by noting that the minimum of a set is less than or equal to the minimum of all elements in the set.
            This leads to the final form of the discrete linear optimisation problem:
{\small 
\abovedisplayskip=8pt
\belowdisplayskip=0pt
\begin{align*}
&\text{maximise}   \  V^{h}(s) \nonumber \\
&\text{subject to}   \nonumber \\
&\  V^{h}(s) \leq \sum_{a_1 \in A_1} Q^{h}(s, a_1, a_2) \cdot m_{a_{1}} \ \forall a_2 \in A_2\\
                          &\  \sum_{a_{1} \in A_1} m_{a_{1}} = 1 ; \  m_{a_{1}} \geq 0 \qquad \forall a_{1} \in A_1 \nonumber
\end{align*}
}%
\end{enumerate}

We can now use existing linear programming solvers to compute the optimal
solution to this linear program for each $s \in S$ at a given horizon $h$.

The linear program used to solve zero-sum DSGs cannot be used once
the state is continuous since there are infinitely many states. The key 
innovation of this paper is in showing that continuous state zero-sum
games can still be solved through the use of symbolic dynamic programming.